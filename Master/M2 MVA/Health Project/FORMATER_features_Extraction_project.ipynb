{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd715151-1c9c-4508-99be-fe3362208803",
      "metadata": {
        "id": "cd715151-1c9c-4508-99be-fe3362208803"
      },
      "source": [
        "## IMPORTER LES LIBRAIRIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3120d5c-e436-4956-8b91-174a5b706740",
      "metadata": {
        "id": "a3120d5c-e436-4956-8b91-174a5b706740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1075ab73-0ccc-4e11-f4fd-7ea1827ede6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "from math import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from numpy.fft import rfft, rfftfreq\n",
        "from itertools import tee\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.kernel_ridge import KernelRidge as KRR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gtOmBFhayWN",
        "outputId": "aa9043fc-370e-438c-97cb-a031a86f5484"
      },
      "id": "5gtOmBFhayWN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POUR COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "PATH = \"/content/gdrive/MyDrive/Projet_PERCY/\"\n",
        "\n",
        "!unzip /content/gdrive/MyDrive/Projet_PERCY/data\n",
        "\n",
        "!cp /content/gdrive/MyDrive/Projet_PERCY/uutils.py /content\n",
        "\n",
        "from uutils import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg517-PjHskB",
        "outputId": "94c6fd2c-e6f9-49d7-bf18-5d2ad82bdef3"
      },
      "id": "wg517-PjHskB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  /content/gdrive/MyDrive/Projet_PERCY/data.zip\n",
            "replace OK2/SCA104_OK2/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a900be85-fbe8-4a1c-8132-08053bd83bb8",
      "metadata": {
        "id": "a900be85-fbe8-4a1c-8132-08053bd83bb8"
      },
      "source": [
        "## FORMATER LES DONNEES"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "personal_data = pd.read_excel(PATH + \"epidemio_et_score.xls\", header = 1, skiprows=8, index_col = 0)\n",
        "\n",
        "# get current directory\n",
        "path = os.getcwd()\n",
        "print(\"Current Directory\", path)\n",
        " \n",
        "# parent directory\n",
        "parent = os.path.dirname(path)\n",
        "print(\"Parent directory\", parent)\n",
        "\n",
        "regard_dictionnaire = {\"regard_contraint\": \"R\", \"regard_libre\": \"S\"}\n",
        "sensors_dictionnaire = {\"tete\": \"00B4242D.txt\", \"tronc\": \"00B42409.txt\", \"pied_gauche\": \"00B4220C.txt\", \"pied_droit\": \"00B4220F.txt\"}\n",
        "signal_dictionnaire = {\"acceleration\": \"Acc_\", \"free_acceleration\": \"FreeAcc_\", \"gyration\": \"Gyr_\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVCEd7e6uUpL",
        "outputId": "5f583e88-28cb-453d-de49-1ab496352723"
      },
      "id": "KVCEd7e6uUpL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory /content\n",
            "Parent directory /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAITEMENT DES VARIABLES CATEGORIELLES"
      ],
      "metadata": {
        "id": "gsa-D7yStafh"
      },
      "id": "gsa-D7yStafh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19c2f6f-239b-40b4-bb98-f053bf001418",
      "metadata": {
        "id": "d19c2f6f-239b-40b4-bb98-f053bf001418"
      },
      "outputs": [],
      "source": [
        "personal_data.loc[personal_data[\"Heures de vol sur les 15 derniers jours\"]==\"X\",\"Heures de vol sur les 15 derniers jours\"] = int(0)\n",
        "personal_data.loc[personal_data[\"Lieu de mission sur les 6 derniers mois\"]==0,\"Lieu de mission sur les 6 derniers mois\"] = 'X'\n",
        "personal_data[\"Lieu de mission sur les 6 derniers mois\"] = personal_data[\"Lieu de mission sur les 6 derniers mois\"].fillna(\"X\")\n",
        "personal_data.loc[personal_data[\"Lequelles ? (sport, arts, multimédia, tech. de relaxation… )\"]==0,\"Lequelles ? (sport, arts, multimédia, tech. de relaxation… )\"] = 'X'\n",
        "personal_data.loc[personal_data[\"Type de sport ( force /endurance)\"]==0,\"Type de sport ( force /endurance)\"] = 'X'\n",
        "personal_data[\"Prise de médicaments sur les 15 derniers jours ? \"] = personal_data[\"Prise de médicaments sur les 15 derniers jours ? \"].fillna(0)\n",
        "personal_data[\"Quellle classe thérapeutique ? \"] = personal_data[\"Quellle classe thérapeutique ? \"].fillna(\"X\")\n",
        "personal_data.loc[personal_data[\"Quellle classe thérapeutique ? \"]==0,\"Quellle classe thérapeutique ? \"] = \"X\"\n",
        "personal_data.loc[personal_data[\"Sexe\"]==\"F\",\"Sexe\"] = int(0)\n",
        "personal_data.loc[personal_data[\"Sexe\"]==\"M\",\"Sexe\"] = int(1)\n",
        "personal_data[\"surrentrainement\"] = 0\n",
        "personal_data.loc[personal_data[\"Score SFMS >= 14\"]>=14,\"surrentrainement\"] = 1\n",
        "personal_data.loc[personal_data[\"Temp moyen pour les repas sur les  15J\"]=='> 1h ',\"Temp moyen pour les repas sur les  15J\"] = '> 1h'\n",
        "\n",
        "categorical_features_dummy = [\"Sexe\",\n",
        "                              \"Personnel naviguant ? \",\n",
        "                              \"Barotraumatisme sur les 3 derniers mois ?\",\n",
        "                              \"Prise de médicaments sur les 15 derniers jours ? \",\n",
        "                              \"consomation de tabac  ?\"]  \n",
        "\n",
        "categorical_ordinal = [\"Grade\",\n",
        "                       \"Date dernière mission \",\n",
        "                       \"Temp moyen pour les repas sur les  15J\",\n",
        "                       \"Nombre moyen  de repas par jour sur les 15J\",\n",
        "                       \"Temps de sommeil consécutif moyen 15J\"]\n",
        "\n",
        "for col in categorical_ordinal:\n",
        "    column = personal_data[[col]]\n",
        "    encoder = OrdinalEncoder()\n",
        "    personal_data[col] = encoder.fit_transform(column)                      \n",
        "                          \n",
        "categorical_features_more = [\"Heures de vol sur les 15 derniers jours\",\n",
        "                             \"Lieu de mission sur les 6 derniers mois\",\n",
        "                             \"Automédication ?\",\n",
        "                             \"Réalisation d'activités pour se détendre ?\",\n",
        "                             \"Type de sport ( force /endurance)\",\n",
        "                             'Quellle classe thérapeutique ? ',\n",
        "                             'Lequelles ? (sport, arts, multimédia, tech. de relaxation… )']\n",
        "\n",
        "for col in categorical_features_more:\n",
        "    column = personal_data[col]\n",
        "    features_names = []\n",
        "    for el in np.unique(column):\n",
        "        features_names.append(f\"{col}_{el}\")\n",
        "    features_names = features_names[1:]\n",
        "    new_data = pd.get_dummies(column, drop_first=True)\n",
        "    new_data.columns = features_names\n",
        "    personal_data = pd.concat([personal_data, new_data], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODIFICATIONS A LA MAIN NECESSAIRES :\n",
        "\n",
        "matricule 83NJ :\n",
        "- le fichier SCA83j_OK2 (et tous les sous fichiers) a ete change en SCA83n_OK2 (erreur de nom de fichier)\n",
        "\n",
        "matricule 91ND :\n",
        "- le fichier ecg a ete renomme car erreur de nom : 91NC au lieu de 91ND\n",
        "\n",
        "matricule 106TN :\n",
        "- renommer la colonne dans le ecg . csv (il doit y avoir un pb d encodage)\n",
        "\n",
        "matricule 107SM :\n",
        "- renommer les fichers dans ecg . csv (1O7SM au lieu de 107SM)"
      ],
      "metadata": {
        "id": "5A-bHkA1Yj64"
      },
      "id": "5A-bHkA1Yj64"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pour generer le fichier pickle de donnees formatees, choisir une des 4 cellule suivante a lancer en focntion du pickle souhaité."
      ],
      "metadata": {
        "id": "22I99d6uquP_"
      },
      "id": "22I99d6uquP_"
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise(iterable):\n",
        "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "    \n",
        "def get_fourier_features(signal: np.ndarray, n_bins: int = 1000) -> dict:\n",
        "    \"\"\"The signal is assumed to be centered and scaled to unit variance.\"\"\"\n",
        "    Frequency = 10\n",
        "    n_samples = signal.shape[0]\n",
        "    fourier = abs(rfft(signal))\n",
        "    freqs = rfftfreq(n=n_samples, d=1.0 / Frequency)\n",
        "    res_list = list()\n",
        "    freq_bins = np.linspace(0, Frequency / 2, n_bins + 1)\n",
        "    for (f_min, f_max) in pairwise(freq_bins):\n",
        "        keep = (f_min <= freqs) & (freqs < f_max)\n",
        "        if np.sum(fourier[keep] ** 2) > 0:\n",
        "            res_list.append( np.log(np.sum(fourier[keep] ** 2)))\n",
        "        else: \n",
        "            res_list.append( 0 )\n",
        "    return res_list\n",
        "\n",
        "\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return b, a\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLqd4CgLtH5T"
      },
      "id": "DLqd4CgLtH5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODIF DANS LE BUT DE REGARDER LES DONNEES DE LA MARCHE (DONNEES XCR et XCS) + ECG :\n",
        "\n",
        "# liste des individus ou ca bloque : \n",
        "\n",
        "# matricule 4CS :\n",
        "# pas de fichier ecg .csv : mais un fichier ecg debout + marche .csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"4CS\"]\n",
        "\n",
        "# matricule 17BK :\n",
        "# pas des fichier ecs . csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"17BK\"]\n",
        "\n",
        "# matricule 106TN :\n",
        "# pas des fichier ecs . csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"106TN\"]\n",
        "\n",
        "# matricule 82GA :\n",
        "# pas des fichier de fichier R\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"82GA\"]\n",
        "\n",
        "# matricule 80DD :\n",
        "# que le fichier ECG donc pas de 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"80DD\"]\n",
        "\n",
        "# matricule 87RC :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"87RC\"]\n",
        "\n",
        "# matricule 97RM :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"97RM\"]\n",
        "\n",
        "# matricule 99TB :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"99TB\"]\n",
        "\n",
        "# pas des fichier de fichier R alors que XCR==True\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"82GA\"]\n",
        "\n",
        "# on utilise XCR/XCS :\n",
        "personal_data = personal_data[(personal_data.XCR!=\"N\") & (personal_data.XCS!=\"N\")]\n",
        "\n",
        "# on cree une colonne avec la cohorte\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] >= 14 , \"cohorte\" ] = 'SCA'\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] < 14 , \"cohorte\" ] = 'OK2'\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees pour chaque individu\n",
        "personal_data[\"folder\"] = personal_data.apply(lambda row: row[\"cohorte\" ] + '/SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\"], axis=1)\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees 01_manu_steps.txt pour chaque individu\n",
        "personal_data[\"steps_folder_S\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"S\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "personal_data[\"steps_folder_R\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"R\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "\n",
        "# on cree une colonne avec la liste des coordonnees x des points indiquant les pas pour chaque individu (du fichier 01_manu_steps.txt)\n",
        "personal_data[\"steps_x_coord_S\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_S\"]), axis=1)\n",
        "personal_data[\"steps_x_coord_R\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_R\"]), axis=1)\n",
        "\n",
        "# on cree une colonne qui contient le dictionnaire qui donne pour chaque capteur le nom du fichier dans lequel trouver les donnees pour chaque individu\n",
        "personal_data[\"dict_capteur_S\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "personal_data[\"dict_capteur_R\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "\n",
        "# Setting standard filter requirements.\n",
        "order = 6\n",
        "fs = 30.0\n",
        "cutoff = 3.667\n",
        "\n",
        "# on cree une colonne avec les moyenne / variance des signaux de l acceleration, free_acceleration, gyration\n",
        "dict_capteur_column_S = np.where(personal_data.columns==\"dict_capteur_S\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_S].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[key+ref+\"clean\"] = personal_data.apply(lambda row: butter_lowpass_filter(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref], cutoff, fs, order), axis=1)\n",
        "        personal_data[ key + \"_mean_S_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_S_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_S_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_S_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_freq_S_\" + ref ] = personal_data.apply(lambda row: get_fourier_features( np.array(list(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref])) ), axis=1)\n",
        "\n",
        "dict_capteur_column_R = np.where(personal_data.columns==\"dict_capteur_R\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_R].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[key+ref+\"clean\"] = personal_data.apply(lambda row: butter_lowpass_filter(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref], cutoff, fs, order), axis=1)\n",
        "        personal_data[ key + \"_mean_R_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_R_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_R_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_R_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_freq_R_\" + ref ] = personal_data.apply(lambda row: get_fourier_features( np.array(list(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref])) ), axis=1)\n",
        "\n",
        "personal_data[\"ecg_folder\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_ecg/' + (row[\"Numéro inclusion\"]) + '.csv', axis=1)\n",
        "personal_data[\"ecg_data\"] = personal_data.apply(lambda row: ECG_RR(row[\"ecg_folder\"]), axis=1)\n",
        "\n",
        "personal_data[\"ecg_mean\"] = personal_data.apply(lambda row: np.nanmean(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_std\"] = personal_data.apply(lambda row: np.nanstd(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_min\"] = personal_data.apply(lambda row: np.nanmin(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_max\"] = personal_data.apply(lambda row: np.nanmax(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_acf\"] = personal_data.apply(lambda row: acf(row[\"ecg_data\"], missing=\"conservative\"), axis=1)\n",
        "\n",
        "steps_S  = personal_data.steps_x_coord_S.to_numpy()\n",
        "max_step_S = np.max(np.max(steps_S))\n",
        "min_step_S = np.min(np.min(np.min(steps_S)))\n",
        "steps_right_S = [ (steps_S[i][0] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_left_S = [ (steps_S[i][1] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_right_S_mean = np.array([np.mean(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_right_S_std = np.array([np.std(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_left_S_mean = np.array([np.mean(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_left_S_std = np.array([np.std(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_diff_S = abs(steps_right_S_mean - steps_left_S_mean)\n",
        "\n",
        "personal_data[\"steps_right_S_mean\"] = steps_right_S_mean\n",
        "personal_data[\"steps_right_S_std\"] = steps_right_S_std\n",
        "personal_data[\"steps_left_S_mean\"] = steps_left_S_mean\n",
        "personal_data[\"steps_left_S_std\"] = steps_left_S_std\n",
        "personal_data[\"steps_diff_S\"] = steps_diff_S\n",
        "\n",
        "steps_R  = personal_data.steps_x_coord_R.to_numpy()\n",
        "max_step_R = np.max(np.max(steps_R))\n",
        "min_step_R = np.min(np.min(np.min(steps_R)))\n",
        "steps_right_R = [ (steps_R[i][0] - min_step_R)/(max_step_R - min_step_R) for i in range(len(steps_R))]\n",
        "steps_left_R = [ (steps_R[i][1] - min_step_R)/(max_step_R - min_step_R) for i in range(len(steps_R))]\n",
        "steps_right_R_mean = np.array([np.mean(steps_right_R[i])  for i in range(len(steps_right_R))])\n",
        "steps_right_R_std = np.array([np.std(steps_right_R[i])  for i in range(len(steps_right_R))])\n",
        "steps_left_R_mean = np.array([np.mean(steps_left_R[i])  for i in range(len(steps_left_R))])\n",
        "steps_left_R_std = np.array([np.std(steps_left_R[i])  for i in range(len(steps_left_R))])\n",
        "steps_diff_R = abs(steps_right_R_mean - steps_left_R_mean)\n",
        "\n",
        "personal_data[\"steps_right_R_mean\"] = steps_right_R_mean\n",
        "personal_data[\"steps_right_R_std\"] = steps_right_R_std\n",
        "personal_data[\"steps_left_R_mean\"] = steps_left_R_mean\n",
        "personal_data[\"steps_left_R_std\"] = steps_left_R_std\n",
        "personal_data[\"steps_diff_R\"] = steps_diff_R\n",
        "\n",
        "personal_data[\"steps_right_diffSR_mean\"] = abs(steps_right_R_mean - steps_right_S_mean)\n",
        "personal_data[\"steps_right_diffSR_std\"] = abs(steps_right_R_std - steps_right_S_std)\n",
        "personal_data[\"steps_left_diffSR_mean\"] = abs(steps_left_R_mean - steps_left_S_mean)\n",
        "personal_data[\"steps_left_diffSR_std\"] = abs(steps_left_R_std - steps_left_S_std)\n",
        "personal_data[\"steps_diff_SR\"] = abs(steps_diff_R - steps_diff_S)\n",
        "\n",
        "# personal_data.to_pickle(PATH + \"complete_personal_data_marche_XCR_XCS_ECG.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIWlSbeouJdx",
        "outputId": "bbeed558-774b-4554-8090-58ab112586c7"
      },
      "id": "QIWlSbeouJdx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: DtypeWarning: Columns (0,1,2,3,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: DtypeWarning: Columns (0,1,2,3,16,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: DtypeWarning: Columns (0,1,2,3,6,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: DtypeWarning: Columns (0,1,2,3,10,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: RuntimeWarning: Mean of empty slice\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/nanfunctions.py:1671: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  keepdims=keepdims)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: RuntimeWarning: All-NaN axis encountered\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: RuntimeWarning: All-NaN axis encountered\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/stattools.py:541: FutureWarning: fft=True will become the default in a future version of statsmodels. To suppress this warning, explicitly set fft=False.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/stattools.py:382: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  xo = x - x.sum() / notmask_int.sum()\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/stattools.py:437: RuntimeWarning: invalid value encountered in true_divide\n",
            "  acov = np.correlate(xo, xo, 'full')[n - 1:] / d[n - 1:]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "personal_data.to_pickle(PATH + \"complete_personal_data_marche_XCR_XCS_ECG.pkl\")"
      ],
      "metadata": {
        "id": "-oZ_w798-42J"
      },
      "id": "-oZ_w798-42J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODIF DANS LE BUT DE REGARDER LES DONNEES DE LA MARCHE (DONNEES XCR et XCS):\n",
        "\n",
        "# liste des individus ou ca bloque : \n",
        "\n",
        "# matricule 80DD :\n",
        "# que le fichier ECG donc pas de 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"80DD\"]\n",
        "\n",
        "# matricule 87RC :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"87RC\"]\n",
        "\n",
        "# matricule 97RM :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"97RM\"]\n",
        "\n",
        "# matricule 99TB :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"99TB\"]\n",
        "\n",
        "# pas des fichier de fichier R alors que XCR==True\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"82GA\"]\n",
        "\n",
        "# on utilise XCR/XCS :\n",
        "personal_data = personal_data[(personal_data.XCR!=\"N\") & (personal_data.XCS!=\"N\")]\n",
        "\n",
        "# on cree une colonne avec la cohorte\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] >= 14 , \"cohorte\" ] = 'SCA'\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] < 14 , \"cohorte\" ] = 'OK2'\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees pour chaque individu\n",
        "personal_data[\"folder\"] = personal_data.apply(lambda row: row[\"cohorte\" ] + '/SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\"], axis=1)\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees 01_manu_steps.txt pour chaque individu\n",
        "personal_data[\"steps_folder_S\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"S\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "personal_data[\"steps_folder_R\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"R\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "\n",
        "# on cree une colonne avec la liste des coordonnees x des points indiquant les pas pour chaque individu (du fichier 01_manu_steps.txt)\n",
        "personal_data[\"steps_x_coord_S\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_S\"]), axis=1)\n",
        "personal_data[\"steps_x_coord_R\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_R\"]), axis=1)\n",
        "\n",
        "# on cree une colonne qui contient le dictionnaire qui donne pour chaque capteur le nom du fichier dans lequel trouver les donnees pour chaque individu\n",
        "personal_data[\"dict_capteur_S\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "personal_data[\"dict_capteur_R\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"R\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "\n",
        "# on cree une colonne avec les moyenne / variance des signaux de l acceleration, free_acceleration, gyration\n",
        "dict_capteur_column_S = np.where(personal_data.columns==\"dict_capteur_S\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_S].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[ key + \"_mean_S_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_S_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_S_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_S_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "\n",
        "dict_capteur_column_R = np.where(personal_data.columns==\"dict_capteur_R\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_R].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[ key + \"_mean_R_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_R_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_R_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_R_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_R\"][key])[ref]), axis=1)\n",
        "\n",
        "steps_S  = personal_data.steps_x_coord_S.to_numpy()\n",
        "max_step_S = np.max(np.max(steps_S))\n",
        "min_step_S = np.min(np.min(np.min(steps_S)))\n",
        "steps_right_S = [ (steps_S[i][0] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_left_S = [ (steps_S[i][1] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_right_S_mean = np.array([np.mean(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_right_S_std = np.array([np.std(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_left_S_mean = np.array([np.mean(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_left_S_std = np.array([np.std(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_diff_S = abs(steps_right_S_mean - steps_left_S_mean)\n",
        "\n",
        "personal_data[\"steps_right_S_mean\"] = steps_right_S_mean\n",
        "personal_data[\"steps_right_S_std\"] = steps_right_S_std\n",
        "personal_data[\"steps_left_S_mean\"] = steps_left_S_mean\n",
        "personal_data[\"steps_left_S_std\"] = steps_left_S_std\n",
        "personal_data[\"steps_diff_S\"] = steps_diff_S\n",
        "\n",
        "steps_R  = personal_data.steps_x_coord_R.to_numpy()\n",
        "max_step_R = np.max(np.max(steps_R))\n",
        "min_step_R = np.min(np.min(np.min(steps_R)))\n",
        "steps_right_R = [ (steps_R[i][0] - min_step_R)/(max_step_R - min_step_R) for i in range(len(steps_R))]\n",
        "steps_left_R = [ (steps_R[i][1] - min_step_R)/(max_step_R - min_step_R) for i in range(len(steps_R))]\n",
        "steps_right_R_mean = np.array([np.mean(steps_right_R[i])  for i in range(len(steps_right_R))])\n",
        "steps_right_R_std = np.array([np.std(steps_right_R[i])  for i in range(len(steps_right_R))])\n",
        "steps_left_R_mean = np.array([np.mean(steps_left_R[i])  for i in range(len(steps_left_R))])\n",
        "steps_left_R_std = np.array([np.std(steps_left_R[i])  for i in range(len(steps_left_R))])\n",
        "steps_diff_R = abs(steps_right_R_mean - steps_left_R_mean)\n",
        "\n",
        "personal_data[\"steps_right_R_mean\"] = steps_right_R_mean\n",
        "personal_data[\"steps_right_R_std\"] = steps_right_R_std\n",
        "personal_data[\"steps_left_R_mean\"] = steps_left_R_mean\n",
        "personal_data[\"steps_left_R_std\"] = steps_left_R_std\n",
        "personal_data[\"steps_diff_R\"] = steps_diff_R\n",
        "\n",
        "personal_data[\"steps_right_diffSR_mean\"] = abs(steps_right_R_mean - steps_right_S_mean)\n",
        "personal_data[\"steps_right_diffSR_std\"] = abs(steps_right_R_std - steps_right_S_std)\n",
        "personal_data[\"steps_left_diffSR_mean\"] = abs(steps_left_R_mean - steps_left_S_mean)\n",
        "personal_data[\"steps_left_diffSR_std\"] = abs(steps_left_R_std - steps_left_S_std)\n",
        "personal_data[\"steps_diff_SR\"] = abs(steps_diff_R - steps_diff_S)\n",
        "\n",
        "personal_data.to_pickle(PATH + \"complete_personal_data_marche_XCR_XCS.pkl\")"
      ],
      "metadata": {
        "id": "64s_MT_takLj"
      },
      "id": "64s_MT_takLj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODIF DANS LE BUT DE REGARDER LES DONNEES DE LA MARCHE (XCS):\n",
        "\n",
        "# liste des individus ou ca bloque : \n",
        "\n",
        "# matricule 80DD :\n",
        "# que le fichier ECG donc pas de 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"80DD\"]\n",
        "\n",
        "# matricule 87RC :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"87RC\"]\n",
        "\n",
        "# matricule 97RM :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"97RM\"]\n",
        "\n",
        "# matricule 99TB :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"99TB\"]\n",
        "\n",
        "# pas des fichier de fichier R alors que XCR==True\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"82GA\"]\n",
        "\n",
        "# on utilise XCS :\n",
        "personal_data = personal_data[personal_data.XCS==\"O\"]\n",
        "\n",
        "# on cree une colonne avec la cohorte\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] >= 14 , \"cohorte\" ] = 'SCA'\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] < 14 , \"cohorte\" ] = 'OK2'\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees pour chaque individu\n",
        "personal_data[\"folder\"] = personal_data.apply(lambda row: row[\"cohorte\" ] + '/SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\"], axis=1)\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees 01_manu_steps.txt pour chaque individu\n",
        "personal_data[\"steps_folder_S\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"S\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "\n",
        "# on cree une colonne avec la liste des coordonnees x des points indiquant les pas pour chaque individu (du fichier 01_manu_steps.txt)\n",
        "personal_data[\"steps_x_coord_S\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_S\"]), axis=1)\n",
        "\n",
        "# on cree une colonne qui contient le dictionnaire qui donne pour chaque capteur le nom du fichier dans lequel trouver les donnees pour chaque individu\n",
        "personal_data[\"dict_capteur_S\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "\n",
        "# on cree une colonne avec les moyenne / variance des signaux de l acceleration, free_acceleration, gyration\n",
        "dict_capteur_column_S = np.where(personal_data.columns==\"dict_capteur_S\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_S].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[ key + \"_mean_S_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_S_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_S_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_S_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "\n",
        "steps_S  = personal_data.steps_x_coord_S.to_numpy()\n",
        "max_step_S = np.max(np.max(steps_S))\n",
        "min_step_S = np.min(np.min(np.min(steps_S)))\n",
        "steps_right_S = [ (steps_S[i][0] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_left_S = [ (steps_S[i][1] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_right_S_mean = np.array([np.mean(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_right_S_std = np.array([np.std(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_left_S_mean = np.array([np.mean(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_left_S_std = np.array([np.std(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_diff_S = abs(steps_right_S_mean - steps_left_S_mean)\n",
        "\n",
        "personal_data[\"steps_right_S_mean\"] = steps_right_S_mean\n",
        "personal_data[\"steps_right_S_std\"] = steps_right_S_std\n",
        "personal_data[\"steps_left_S_mean\"] = steps_left_S_mean\n",
        "personal_data[\"steps_left_S_std\"] = steps_left_S_std\n",
        "personal_data[\"steps_diff_S\"] = steps_diff_S\n",
        "\n",
        "personal_data.to_pickle(PATH + \"complete_personal_data_marche_XCS.pkl\")"
      ],
      "metadata": {
        "id": "ZsDG2OuXc1es"
      },
      "id": "ZsDG2OuXc1es",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODIF DANS LE BUT DE REGARDER LES DONNEES DES ECG + LA MARCHE (XCS):\n",
        "\n",
        "# liste des individus ou ca bloque : \n",
        "\n",
        "# matricule 80DD :\n",
        "# que le fichier ECG\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"80DD\"]\n",
        "\n",
        "# matricule 87RC :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"87RC\"]\n",
        "\n",
        "# matricule 97RM :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"97RM\"]\n",
        "\n",
        "# matricule 99TB :\n",
        "# pas de fichier avec les points du type 01_manu_steps.txt\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"99TB\"]\n",
        "\n",
        "# matricule 4CS :\n",
        "# pas de fichier ecg .csv : mais un fichier ecg debout + marche .csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"4CS\"]\n",
        "\n",
        "# matricule 17BK :\n",
        "# pas des fichier ecs . csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"17BK\"]\n",
        "\n",
        "# matricule 106TN :\n",
        "# pas des fichier ecs . csv\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"106TN\"]\n",
        "\n",
        "# matricule 82GA :\n",
        "# pas des fichier de fichier R\n",
        "personal_data = personal_data[personal_data[\"Numéro inclusion\"] != \"82GA\"]\n",
        "\n",
        "# on utilise XCS :\n",
        "personal_data = personal_data[personal_data.XCS==\"O\"]\n",
        "\n",
        "# on cree une colonne avec la cohorte\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] >= 14 , \"cohorte\" ] = 'SCA'\n",
        "personal_data.loc[ personal_data[\"Score SFMS >= 14\"] < 14 , \"cohorte\" ] = 'OK2'\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees pour chaque individu\n",
        "personal_data[\"folder\"] = personal_data.apply(lambda row: row[\"cohorte\" ] + '/SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\"], axis=1)\n",
        "\n",
        "# on cree une colonne avec le nom du folder dans lequel se trouvent les donnees 01_manu_steps.txt pour chaque individu\n",
        "personal_data[\"steps_folder_S\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_00000000_XC' + \"S\" + '_01_' + 'manu_steps.txt', axis=1)\n",
        "\n",
        "# on cree une colonne avec la liste des coordonnees x des points indiquant les pas pour chaque individu (du fichier 01_manu_steps.txt)\n",
        "personal_data[\"steps_x_coord_S\"] = personal_data.apply(lambda row: steps_manu_x(row[\"steps_folder_S\"]), axis=1)\n",
        "\n",
        "# on cree une colonne qui contient le dictionnaire qui donne pour chaque capteur le nom du fichier dans lequel trouver les donnees pour chaque individu\n",
        "personal_data[\"dict_capteur_S\"] = personal_data.apply(lambda row: \\\n",
        "    {\"nom_tete\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tete'], \\\n",
        "    \"nom_tronc\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['tronc'], \\\n",
        "    \"nom_pied_gauche\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_gauche'], \\\n",
        "    \"nom_pied_droit\" : 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + \\\n",
        "     '_00000000_XC' + \"S\" + '_01_' + sensors_dictionnaire['pied_droit'] }, axis=1)\n",
        "\n",
        "# on cree une colonne avec les moyenne / variance des signaux de l acceleration, free_acceleration, gyration\n",
        "dict_capteur_column_S = np.where(personal_data.columns==\"dict_capteur_S\")[0].item()\n",
        "for key in personal_data.iloc[0,dict_capteur_column_S].keys():\n",
        "    for ref in [\"Acc_X\", \"Acc_Y\", \"Acc_Z\", \"FreeAcc_X\", \"FreeAcc_Y\", \"FreeAcc_Z\", \"Gyr_X\", \"Gyr_Y\", \"Gyr_Z\"]:\n",
        "        personal_data[ key + \"_mean_S_\" + ref ] = personal_data.apply(lambda row: np.mean(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_std_S_\" + ref ] = personal_data.apply(lambda row: np.std(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_min_S_\" + ref ] = personal_data.apply(lambda row: np.min(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "        personal_data[ key + \"_max_S_\" + ref ] = personal_data.apply(lambda row: np.max(importfile_XSens(row[\"folder\" ] + '/' + row[\"dict_capteur_S\"][key])[ref]), axis=1)\n",
        "\n",
        "personal_data[\"ecg_folder\"] = personal_data.apply(lambda row: row[\"folder\" ] + '/' + 'SCA' + (row[\"Numéro inclusion\"][0:3]).lower() + '_' + row[\"cohorte\" ] + '_ecg/' + (row[\"Numéro inclusion\"]) + '.csv', axis=1)\n",
        "personal_data[\"ecg_data\"] = personal_data.apply(lambda row: ECG_RR(row[\"ecg_folder\"]), axis=1)\n",
        "\n",
        "personal_data[\"ecg_mean\"] = personal_data.apply(lambda row: np.nanmean(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_std\"] = personal_data.apply(lambda row: np.nanstd(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_min\"] = personal_data.apply(lambda row: np.nanmin(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_max\"] = personal_data.apply(lambda row: np.nanmax(row[\"ecg_data\"]), axis=1)\n",
        "personal_data[\"ecg_acf\"] = personal_data.apply(lambda row: acf(row[\"ecg_data\"], missing=\"conservative\"), axis=1)\n",
        "\n",
        "steps_S  = personal_data.steps_x_coord_S.to_numpy()\n",
        "max_step_S = np.max(np.max(steps_S))\n",
        "min_step_S = np.min(np.min(np.min(steps_S)))\n",
        "steps_right_S = [ (steps_S[i][0] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_left_S = [ (steps_S[i][1] - min_step_S)/(max_step_S - min_step_S) for i in range(len(steps_S))]\n",
        "steps_right_S_mean = np.array([np.mean(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_right_S_std = np.array([np.std(steps_right_S[i])  for i in range(len(steps_right_S))])\n",
        "steps_left_S_mean = np.array([np.mean(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_left_S_std = np.array([np.std(steps_left_S[i])  for i in range(len(steps_left_S))])\n",
        "steps_diff_S = abs(steps_right_S_mean - steps_left_S_mean)\n",
        "\n",
        "personal_data[\"steps_right_S_mean\"] = steps_right_S_mean\n",
        "personal_data[\"steps_right_S_std\"] = steps_right_S_std\n",
        "personal_data[\"steps_left_S_mean\"] = steps_left_S_mean\n",
        "personal_data[\"steps_left_S_std\"] = steps_left_S_std\n",
        "personal_data[\"steps_diff_S\"] = steps_diff_S\n",
        "\n",
        "personal_data.to_pickle(PATH + \"complete_personal_data_marche_XCS_ECG.pkl\")"
      ],
      "metadata": {
        "id": "5UPqlXmXc9-X"
      },
      "id": "5UPqlXmXc9-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-yJEfVPkmzVf"
      },
      "id": "-yJEfVPkmzVf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "FORMATER features_Extraction_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}