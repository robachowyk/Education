{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_2021_Lab1_HAN_Handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVouapRmjEW"
      },
      "source": [
        "<center><h2>ALTeGraD 2021<br>Lab Session 1: HAN</h2><h3>Hierarchical Attention Network Using GRU</h3> 09 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogg6hkhFW8Jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c1d247-a3f8-4c9b-e838-80c1a4ed0a74"
      },
      "source": [
        "# In case you are using google colab:\n",
        "# uncomment the following two lines: \n",
        "\n",
        "%tensorflow_version 1.9\n",
        "!pip install keras==2.2.5\n",
        "\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\" -O \"data.zip\"\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.9`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n",
            "--2021-11-14 17:57:37--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vqtlqw.am.files.1drv.com/y4mZbvz3OkB6D0s40RAv37ibN9RLMlNvh5Hmtbzqj6bq1b5N3k3ediK0IFK82z6lUmOyjFuxq4j-a6xIEvfl083tQWiIoht2ZhX0n0DBS4SadxbD7SuM3XEF7wWOZiI166wQkYbQGGk4yFA8QMt9-S06stW2UymvSDUq0_X6jnmTXY3KCZUyOFjZCt4HTUzExoR8BfNdBLI71micHmgwWDvEQ/data.zip?download&psid=1 [following]\n",
            "--2021-11-14 17:57:38--  https://vqtlqw.am.files.1drv.com/y4mZbvz3OkB6D0s40RAv37ibN9RLMlNvh5Hmtbzqj6bq1b5N3k3ediK0IFK82z6lUmOyjFuxq4j-a6xIEvfl083tQWiIoht2ZhX0n0DBS4SadxbD7SuM3XEF7wWOZiI166wQkYbQGGk4yFA8QMt9-S06stW2UymvSDUq0_X6jnmTXY3KCZUyOFjZCt4HTUzExoR8BfNdBLI71micHmgwWDvEQ/data.zip?download&psid=1\n",
            "Resolving vqtlqw.am.files.1drv.com (vqtlqw.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to vqtlqw.am.files.1drv.com (vqtlqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Archive:  data.zip\n",
            "replace __MACOSX/._data? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJaSJaIP1xRy"
      },
      "source": [
        "# = = = = = Attention Layer = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeXk--0rncj4"
      },
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    https://github.com/richliao/textClassifier/issues/13#issuecomment-377323318\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoM7H0KQncpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5525c4e3-d6ed-4e10-febb-6da7c916c9b8"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.layers import Layer as Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    \n",
        "    How to use:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    \n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, return_coefficients=False,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.return_coefficients = return_coefficients\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "        \n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "        \n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "        \n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        \n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "        \n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "    \n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "    \n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "        \n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "        \n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "        \n",
        "        a = K.exp(ait)\n",
        "        \n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        \n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        \n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        ### fill the gap ### # compute the attentional vector\n",
        "        weighted_input = K.sum(a*x, axis = 1)\n",
        "        \n",
        "        if self.return_coefficients:\n",
        "          ### fill the gap - [attentional vector, coefficients] ###\n",
        "            return  [weighted_input, a]\n",
        "        else:\n",
        "          ### fill the gap - attentional vector only ###\n",
        "            return  weighted_input\n",
        "    \n",
        "    \n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_coefficients:\n",
        "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB-WOy4YHlsr"
      },
      "source": [
        "# = = = = = Bidirectional GRU = = = = =\n",
        "#### fill the gaps in the bidir_gru function below ###\n",
        "#### add a RNN-GRU layer and a bidirectional wrapper ###\n",
        "#### bidirectional: search for 'bidirectional' [here](https://keras.io/layers/wrappers/)\n",
        "#### GRU: search for 'GRU' [here](https://keras.io/layers/recurrent/)\n",
        "#### layers can be combined by nesting them as: layer_b(parameters_b)(layer_a(parameters_a)(input))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoFkuGwNncwy"
      },
      "source": [
        "from keras.layers import Bidirectional, GRU\n",
        "\n",
        "def bidir_gru(my_seq,n_units):\n",
        "    '''\n",
        "    just a convenient wrapper for bidirectional RNN with GRU units\n",
        "    '''\n",
        "    layer = GRU(units=n_units, activation=\"tanh\", return_sequences=True)\n",
        "\n",
        "    return Bidirectional(layer = layer, merge_mode = \"sum\")(my_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTP6GrOHlss"
      },
      "source": [
        "# = = = = = Parameters = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czsVjxgYnczb"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "path_root = ''\n",
        "path_to_data = path_root + 'data/'\n",
        "\n",
        "d = 30 # dimensionality of word embeddings\n",
        "n_units = 50 # RNN layer dimensionality\n",
        "drop_rate = 0.5 # dropout\n",
        "mfw_idx = 2 # index of the most frequent words in the dictionary \n",
        "            # 0 is for the special padding token\n",
        "            # 1 is for the special out-of-vocabulary token\n",
        "\n",
        "padding_idx = 0\n",
        "oov_idx = 1\n",
        "batch_size = 32\n",
        "nb_epochs = 6\n",
        "my_optimizer = 'adam'\n",
        "my_patience = 2 # for early stopping strategy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Vot_C7Hlst"
      },
      "source": [
        "# = = = = = Data Loading = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD6hRh0OHlst"
      },
      "source": [
        "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
        "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
        "\n",
        "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
        "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
        "\n",
        "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
        "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
        "    word_to_index = json.load(my_file)\n",
        "\n",
        "# invert mapping\n",
        "index_to_word = dict((v,k) for k,v in word_to_index.items()) ### fill the gap (use a dict comprehension) ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rzqEGOdHlst"
      },
      "source": [
        "# = = = = = Defining Architecture = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMj9j1_pHlst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72490714-a085-45d5-c68b-97c848ab934d"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dropout, TimeDistributed, Dense\n",
        "\n",
        "sent_ints = Input(shape=(my_docs_array_train.shape[2],)) # vec of ints of variable size\n",
        "\n",
        "sent_wv = Embedding(input_dim=len(index_to_word)+2, # vocab size\n",
        "                    output_dim=d, # dimensionality of embedding space\n",
        "                    input_length=my_docs_array_train.shape[2],\n",
        "                    trainable=True\n",
        "                    )(sent_ints)\n",
        "\n",
        "sent_wv_dr = Dropout(drop_rate)(sent_wv)\n",
        "\n",
        "### fill the gap ### # get the annotations for each word in the sent\n",
        "sent_wa = bidir_gru(sent_wv_dr, n_units)\n",
        "\n",
        "### fill the gap ### # get the attentional vector for the sentence\n",
        "sent_att_vec, word_att_coeffs = AttentionWithContext(return_coefficients=True)(sent_wa)\n",
        "sent_att_vec_dr = Dropout(drop_rate)(sent_att_vec)                      \n",
        "sent_encoder = Model(sent_ints,sent_att_vec_dr)# return att vector not coef\n",
        "\n",
        "doc_ints = Input(shape=(my_docs_array_train.shape[1],my_docs_array_train.shape[2],))\n",
        "### fill the gap ### # apply the sentence encoder model to each sentence in the document. Search for 'TimeDistributed' in https://keras.io/layers/wrappers/\n",
        "sent_att_vecs_dr = TimeDistributed(sent_encoder)(doc_ints)\n",
        "doc_sa = bidir_gru(sent_att_vecs_dr,n_units) ### fill the gap ### # get annotations for each sent in the doc\n",
        "### fill the gap ### # get attentional vector for the doc\n",
        "doc_att_vec, sent_att_coeffs = AttentionWithContext(return_coefficients=True)(doc_sa)\n",
        "doc_att_vec_dr = Dropout(drop_rate)(doc_att_vec)\n",
        "                  \n",
        "preds = Dense(units=1,\n",
        "              activation='sigmoid')(doc_att_vec_dr)\n",
        "\n",
        "model = Model(doc_ints,preds)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = my_optimizer,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print('model compiled')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgreR5AcHlst"
      },
      "source": [
        "# = = = = = Training = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbW_vheGHlst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8161c33-c3bd-48f2-fe1b-5259ed69bf88"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "loading_pretrained = False\n",
        "\n",
        "if not loading_pretrained:\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_acc',\n",
        "                                   patience=my_patience,\n",
        "                                   mode='max')\n",
        "    \n",
        "    # save model corresponding to best epoch\n",
        "    checkpointer = ModelCheckpoint(filepath=path_to_data + 'model', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True)\n",
        "    \n",
        "    # 200s/epoch on CPU - reaches 84.38% accuracy in 2 epochs\n",
        "\n",
        "    ### fill the gap ### \n",
        "    # call the .fit() method on your model with the arguments: \n",
        "    # my_docs_array_train, my_labels_array_train, batch_size, nb_epochs, my_docs_array_test, my_labels_array_test, early_stopping\n",
        "    # look at: https://keras.io/models/sequential/#fit\n",
        "\n",
        "    model.fit(x = my_docs_array_train, \n",
        "              y = my_labels_array_train, \n",
        "              batch_size = batch_size, \n",
        "              epochs=nb_epochs, \n",
        "              validation_data = (my_docs_array_test, my_labels_array_test), \n",
        "              callbacks = [early_stopping, checkpointer]) # checkpointer\n",
        "\n",
        "else:\n",
        "    model.load_weights(path_to_data + 'model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "25000/25000 [==============================] - 116s 5ms/step - loss: 0.4538 - acc: 0.7717 - val_loss: 0.3717 - val_acc: 0.8356\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.37172, saving model to data/model\n",
            "Epoch 2/6\n",
            "25000/25000 [==============================] - 113s 5ms/step - loss: 0.2653 - acc: 0.8944 - val_loss: 0.3947 - val_acc: 0.8276\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.37172\n",
            "Epoch 3/6\n",
            "25000/25000 [==============================] - 113s 5ms/step - loss: 0.1881 - acc: 0.9278 - val_loss: 0.4416 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.37172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvyr8B5QHlst"
      },
      "source": [
        "# = = = = = Extraction of Attention Coefficients = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr8cS4MHlst"
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "# define intermediate models\n",
        "### fill the two gaps below ###\n",
        "\n",
        "get_word_att_coeffs = Model(sent_ints, word_att_coeffs)\n",
        "# attention coeffs over the words in a sent\n",
        "\n",
        "get_sent_attention_coeffs = Model(doc_ints, sent_att_coeffs)\n",
        "# attention coeffs over the sents in the doc\n",
        "\n",
        "my_review = my_docs_array_test[-1:,:,:] # select last review\n",
        "# convert integer review to text\n",
        "index_to_word[1] = 'OOV'\n",
        "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHDJ7JiqHlsu"
      },
      "source": [
        "# = = = = = Attention Over Sentences in the Document = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yooWg3kkHlsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cfd5a6-b05a-4edc-8b7a-354184cb4102"
      },
      "source": [
        "sent_coeffs = get_sent_attention_coeffs.predict(my_review)\n",
        "sent_coeffs = sent_coeffs[0,:,:]\n",
        "\n",
        "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
        "    print(round(elt[0]*100,2),elt[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.41 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n",
            "14.06 Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot\n",
            "11.18 As others have pointed out , one single viewing of this movie is not sufficient .\n",
            "14.39 If you have the DVD of MD , you can OOV ' by looking at David Lynch 's 'Top 10 OOV to OOV MD ' ( but only upon second\n",
            "15.27 ; ) First of all , Mulholland Drive is downright brilliant .\n",
            "16.94 A masterpiece .\n",
            "18.75 This is the kind of movie that refuse to leave your head .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rII-DNrKHlsu"
      },
      "source": [
        "# = = = = = Attention Over Words in Each Sentence = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyFjAga6Hlsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13de04c7-d7bd-4551-8bcc-519eb3188b42"
      },
      "source": [
        "from keras.backend.tensorflow_backend import _to_tensor\n",
        "\n",
        "my_review_tensor = _to_tensor(my_review,dtype='float32') # a layer, unlike a model, requires a TensorFlow tensor as input\n",
        "\n",
        "### fill the gap ### # get the word attentional coefficients for each sentence in the document\n",
        "word_coeffs = TimeDistributed(get_word_att_coeffs)(my_review_tensor) # apply the model on each sentence of my review\n",
        "\n",
        "word_coeffs = K.eval(word_coeffs) # shape = (1, 7, 30, 1): (batch size, nb of sents in doc, nb of words per sent, coeff)\n",
        "word_coeffs = word_coeffs[0,:,:,0] # shape = (7, 30) (coeff for each word in each sentence)\n",
        "word_coeffs = sent_coeffs * word_coeffs # re-weight by sentence importance\n",
        "word_coeffs = np.round((word_coeffs*100).astype(np.float64),2)\n",
        "\n",
        "word_coeffs_list = word_coeffs.tolist()\n",
        "\n",
        "# match text and coefficients\n",
        "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
        "\n",
        "for sent in text_word_coeffs:\n",
        "    [print(elt) for elt in sent]  \n",
        "    print('= = = =')\n",
        "\n",
        "# sort words by importance within each sentence\n",
        "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
        "\n",
        "for sent in text_word_coeffs_sorted:\n",
        "    [print(elt) for elt in sent]\n",
        "    print('= = = =')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('There', 0.07)\n",
            "(\"'s\", 0.1)\n",
            "('a', 0.1)\n",
            "('sign', 0.06)\n",
            "('on', 0.15)\n",
            "('The', 0.24)\n",
            "('Lost', 2.98)\n",
            "('Highway', 1.37)\n",
            "('that', 0.57)\n",
            "('says', 0.41)\n",
            "(':', 0.09)\n",
            "('OOV', 0.05)\n",
            "('SPOILERS', 0.05)\n",
            "('OOV', 0.06)\n",
            "('(', 0.09)\n",
            "('but', 0.08)\n",
            "('you', 0.64)\n",
            "('already', 0.31)\n",
            "('knew', 0.63)\n",
            "('that', 0.23)\n",
            "(',', 0.16)\n",
            "('did', 0.14)\n",
            "(\"n't\", 0.05)\n",
            "('you', 0.09)\n",
            "('?', 0.03)\n",
            "(')', 0.21)\n",
            "= = = =\n",
            "('Since', 0.56)\n",
            "('there', 0.48)\n",
            "(\"'s\", 0.43)\n",
            "('a', 0.53)\n",
            "('great', 2.58)\n",
            "('deal', 2.33)\n",
            "('of', 2.57)\n",
            "('people', 2.49)\n",
            "('that', 1.08)\n",
            "('apparently', 0.32)\n",
            "('did', 0.21)\n",
            "('not', 0.07)\n",
            "('get', 0.03)\n",
            "('the', 0.01)\n",
            "('point', 0.0)\n",
            "('of', 0.01)\n",
            "('this', 0.01)\n",
            "('movie', 0.02)\n",
            "(',', 0.02)\n",
            "('I', 0.02)\n",
            "(\"'d\", 0.02)\n",
            "('like', 0.02)\n",
            "('to', 0.02)\n",
            "('contribute', 0.03)\n",
            "('my', 0.06)\n",
            "('interpretation', 0.06)\n",
            "('of', 0.02)\n",
            "('why', 0.02)\n",
            "('the', 0.01)\n",
            "('plot', 0.01)\n",
            "= = = =\n",
            "('As', 1.35)\n",
            "('others', 1.66)\n",
            "('have', 0.49)\n",
            "('pointed', 0.3)\n",
            "('out', 0.33)\n",
            "(',', 0.25)\n",
            "('one', 0.26)\n",
            "('single', 0.38)\n",
            "('viewing', 0.48)\n",
            "('of', 0.19)\n",
            "('this', 0.21)\n",
            "('movie', 0.62)\n",
            "('is', 0.69)\n",
            "('not', 1.31)\n",
            "('sufficient', 0.88)\n",
            "('.', 0.42)\n",
            "= = = =\n",
            "('If', 0.33)\n",
            "('you', 0.48)\n",
            "('have', 0.18)\n",
            "('the', 0.39)\n",
            "('DVD', 0.42)\n",
            "('of', 0.47)\n",
            "('MD', 0.7)\n",
            "(',', 0.26)\n",
            "('you', 0.4)\n",
            "('can', 0.2)\n",
            "('OOV', 0.1)\n",
            "(\"'\", 0.09)\n",
            "('by', 0.12)\n",
            "('looking', 0.12)\n",
            "('at', 0.21)\n",
            "('David', 0.25)\n",
            "('Lynch', 0.3)\n",
            "(\"'s\", 0.6)\n",
            "(\"'Top\", 1.29)\n",
            "('10', 0.61)\n",
            "('OOV', 0.47)\n",
            "('to', 0.68)\n",
            "('OOV', 0.49)\n",
            "('MD', 1.03)\n",
            "(\"'\", 0.33)\n",
            "('(', 0.48)\n",
            "('but', 0.4)\n",
            "('only', 0.91)\n",
            "('upon', 0.9)\n",
            "('second', 1.19)\n",
            "= = = =\n",
            "(';', 0.05)\n",
            "(')', 0.09)\n",
            "('First', 0.05)\n",
            "('of', 0.11)\n",
            "('all', 0.07)\n",
            "(',', 0.06)\n",
            "('Mulholland', 0.11)\n",
            "('Drive', 0.11)\n",
            "('is', 0.1)\n",
            "('downright', 0.07)\n",
            "('brilliant', 10.73)\n",
            "('.', 0.87)\n",
            "= = = =\n",
            "('A', 0.75)\n",
            "('masterpiece', 8.94)\n",
            "('.', 1.58)\n",
            "= = = =\n",
            "('This', 0.98)\n",
            "('is', 2.5)\n",
            "('the', 4.94)\n",
            "('kind', 2.21)\n",
            "('of', 1.44)\n",
            "('movie', 1.46)\n",
            "('that', 0.54)\n",
            "('refuse', 0.38)\n",
            "('to', 0.39)\n",
            "('leave', 0.29)\n",
            "('your', 0.94)\n",
            "('head', 0.77)\n",
            "('.', 0.46)\n",
            "= = = =\n",
            "('Lost', 2.98)\n",
            "('Highway', 1.37)\n",
            "('you', 0.64)\n",
            "('knew', 0.63)\n",
            "('that', 0.57)\n",
            "('says', 0.41)\n",
            "('already', 0.31)\n",
            "('The', 0.24)\n",
            "('that', 0.23)\n",
            "(')', 0.21)\n",
            "(',', 0.16)\n",
            "('on', 0.15)\n",
            "('did', 0.14)\n",
            "(\"'s\", 0.1)\n",
            "('a', 0.1)\n",
            "(':', 0.09)\n",
            "('(', 0.09)\n",
            "('you', 0.09)\n",
            "('but', 0.08)\n",
            "('There', 0.07)\n",
            "('sign', 0.06)\n",
            "('OOV', 0.06)\n",
            "('OOV', 0.05)\n",
            "('SPOILERS', 0.05)\n",
            "(\"n't\", 0.05)\n",
            "('?', 0.03)\n",
            "= = = =\n",
            "('great', 2.58)\n",
            "('of', 2.57)\n",
            "('people', 2.49)\n",
            "('deal', 2.33)\n",
            "('that', 1.08)\n",
            "('Since', 0.56)\n",
            "('a', 0.53)\n",
            "('there', 0.48)\n",
            "(\"'s\", 0.43)\n",
            "('apparently', 0.32)\n",
            "('did', 0.21)\n",
            "('not', 0.07)\n",
            "('my', 0.06)\n",
            "('interpretation', 0.06)\n",
            "('get', 0.03)\n",
            "('contribute', 0.03)\n",
            "('movie', 0.02)\n",
            "(',', 0.02)\n",
            "('I', 0.02)\n",
            "(\"'d\", 0.02)\n",
            "('like', 0.02)\n",
            "('to', 0.02)\n",
            "('of', 0.02)\n",
            "('why', 0.02)\n",
            "('the', 0.01)\n",
            "('of', 0.01)\n",
            "('this', 0.01)\n",
            "('the', 0.01)\n",
            "('plot', 0.01)\n",
            "('point', 0.0)\n",
            "= = = =\n",
            "('others', 1.66)\n",
            "('As', 1.35)\n",
            "('not', 1.31)\n",
            "('sufficient', 0.88)\n",
            "('is', 0.69)\n",
            "('movie', 0.62)\n",
            "('have', 0.49)\n",
            "('viewing', 0.48)\n",
            "('.', 0.42)\n",
            "('single', 0.38)\n",
            "('out', 0.33)\n",
            "('pointed', 0.3)\n",
            "('one', 0.26)\n",
            "(',', 0.25)\n",
            "('this', 0.21)\n",
            "('of', 0.19)\n",
            "= = = =\n",
            "(\"'Top\", 1.29)\n",
            "('second', 1.19)\n",
            "('MD', 1.03)\n",
            "('only', 0.91)\n",
            "('upon', 0.9)\n",
            "('MD', 0.7)\n",
            "('to', 0.68)\n",
            "('10', 0.61)\n",
            "(\"'s\", 0.6)\n",
            "('OOV', 0.49)\n",
            "('you', 0.48)\n",
            "('(', 0.48)\n",
            "('of', 0.47)\n",
            "('OOV', 0.47)\n",
            "('DVD', 0.42)\n",
            "('you', 0.4)\n",
            "('but', 0.4)\n",
            "('the', 0.39)\n",
            "('If', 0.33)\n",
            "(\"'\", 0.33)\n",
            "('Lynch', 0.3)\n",
            "(',', 0.26)\n",
            "('David', 0.25)\n",
            "('at', 0.21)\n",
            "('can', 0.2)\n",
            "('have', 0.18)\n",
            "('by', 0.12)\n",
            "('looking', 0.12)\n",
            "('OOV', 0.1)\n",
            "(\"'\", 0.09)\n",
            "= = = =\n",
            "('brilliant', 10.73)\n",
            "('.', 0.87)\n",
            "('of', 0.11)\n",
            "('Mulholland', 0.11)\n",
            "('Drive', 0.11)\n",
            "('is', 0.1)\n",
            "(')', 0.09)\n",
            "('all', 0.07)\n",
            "('downright', 0.07)\n",
            "(',', 0.06)\n",
            "(';', 0.05)\n",
            "('First', 0.05)\n",
            "= = = =\n",
            "('masterpiece', 8.94)\n",
            "('.', 1.58)\n",
            "('A', 0.75)\n",
            "= = = =\n",
            "('the', 4.94)\n",
            "('is', 2.5)\n",
            "('kind', 2.21)\n",
            "('movie', 1.46)\n",
            "('of', 1.44)\n",
            "('This', 0.98)\n",
            "('your', 0.94)\n",
            "('head', 0.77)\n",
            "('that', 0.54)\n",
            "('.', 0.46)\n",
            "('to', 0.39)\n",
            "('refuse', 0.38)\n",
            "('leave', 0.29)\n",
            "= = = =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D2B7WHyfRnx",
        "outputId": "c0091f14-5755-493c-a75d-541b3886107d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# source of the code : https://gist.github.com/ihsgnef/f13c35cd46624c8f458a4d23589ac768\n",
        "\n",
        "def colorize(words, color_array):\n",
        "    # words is a list of words\n",
        "    # color_array is an array of numbers between 0 and 1 of length equal to words\n",
        "    cmap = matplotlib.cm.get_cmap('Oranges')\n",
        "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
        "    colored_string = ''\n",
        "    for word, color in zip(words, color_array):\n",
        "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
        "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
        "    return colored_string\n",
        "    \n",
        "for sent in text_word_coeffs:\n",
        "\n",
        "    words = [word for (word,coef) in sent]\n",
        "    color_array = [coef for (word,coef) in sent]\n",
        "    s = colorize(words, color_array)\n",
        "\n",
        "    # to display :\n",
        "    from IPython.display import display, HTML\n",
        "    display(HTML(s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #feeddc\">&nbspThere&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee9d4\">&nbsp's&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee9d4\">&nbspa&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffeedd\">&nbspsign&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee2c6\">&nbspon&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd2a6\">&nbspThe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspLost&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspHighway&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f67925\">&nbspthat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fda55f\">&nbspsays&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feead6\">&nbsp:&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffefe0\">&nbspOOV&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffefe0\">&nbspSPOILERS&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffeedd\">&nbspOOV&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feead6\">&nbsp(&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feecd9\">&nbspbut&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ee6511\">&nbspyou&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdc088\">&nbspalready&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f06712\">&nbspknew&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd4aa\">&nbspthat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee0c3\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee4ca\">&nbspdid&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffefe0\">&nbspn't&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feead6\">&nbspyou&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff2e5\">&nbsp?&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd7b1\">&nbsp)&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #f77b28\">&nbspSince&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9344\">&nbspthere&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9f56\">&nbsp's&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fa8532\">&nbspa&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspgreat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspdeal&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbsppeople&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspthat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdbe84\">&nbspapparently&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd7b1\">&nbspdid&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feeddc\">&nbspnot&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff2e5\">&nbspget&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff4e9\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff5eb\">&nbsppoint&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff4e9\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff4e9\">&nbspthis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbspmovie&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbspI&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbsp'd&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbsplike&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbspto&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff2e5\">&nbspcontribute&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffeedd\">&nbspmy&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffeedd\">&nbspinterpretation&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff3e6\">&nbspwhy&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff4e9\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff4e9\">&nbspplot&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspAs&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspothers&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9040\">&nbsphave&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdc38d\">&nbsppointed&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdba7f\">&nbspout&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd0a2\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdce9e\">&nbspone&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdad69\">&nbspsingle&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9344\">&nbspviewing&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fddbb8\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd7b1\">&nbspthis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f26b15\">&nbspmovie&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #e4580a\">&nbspis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspnot&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #a43503\">&nbspsufficient&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fda25a\">&nbsp.&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #fdba7f\">&nbspIf&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9344\">&nbspyou&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fedcbb\">&nbsphave&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdab66\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fda25a\">&nbspDVD&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9547\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #e25508\">&nbspMD&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdce9e\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fda762\">&nbspyou&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd9b4\">&nbspcan&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee9d4\">&nbspOOV&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feead6\">&nbsp'&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee7d0\">&nbspby&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee7d0\">&nbsplooking&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd7b1\">&nbspat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdd0a2\">&nbspDavid&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdc38d\">&nbspLynch&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f3701b\">&nbsp's&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbsp'Top&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f26d17\">&nbsp10&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9547\">&nbspOOV&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #e65a0b\">&nbspto&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9040\">&nbspOOV&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspMD&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdba7f\">&nbsp'&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd9344\">&nbsp(&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fda762\">&nbspbut&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #9b3203\">&nbsponly&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #9e3303\">&nbspupon&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspsecond&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #ffefe0\">&nbsp;&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feead6\">&nbsp)&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffefe0\">&nbspFirst&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee8d2\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feeddc\">&nbspall&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #ffeedd\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee8d2\">&nbspMulholland&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee8d2\">&nbspDrive&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee9d4\">&nbspis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feeddc\">&nbspdownright&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspbrilliant&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #a83703\">&nbsp.&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #d84801\">&nbspA&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspmasterpiece&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbsp.&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"barcode\"; style=\"color: black; background-color: #852904\">&nbspThis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspkind&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #7f2704\">&nbspmovie&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #f9812e\">&nbspthat&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdad69\">&nbsprefuse&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdab66\">&nbspto&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdc590\">&nbspleave&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #912e04\">&nbspyour&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #d04501\">&nbsphead&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fd984b\">&nbsp.&nbsp</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIu4X2TVlTAa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}